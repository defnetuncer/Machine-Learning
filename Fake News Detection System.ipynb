{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HACETTEPE UNIVERSITY DEPARTMENT OF COMPUTER ENGINEERING\n",
    "# BBM409 Fundamentals of  Machine Learning Laboratory\n",
    "### Fall 2018 - Assignment II\n",
    "    NAME AND SURNAME:     Defne Tunçer\n",
    "    STUDENT ID:           21627686\n",
    "    EMAIL:                b21627686@cs.hacettepe.edu.tr\n",
    "    INSTRUCTOR:           Aykut Erdem\n",
    "    TA:                   Necva Bölücü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I THEORY QUESTIONS\n",
    "### MLE\n",
    "• Suppose you have $N$ samples $x_{1}, x_{2}.....x_{N}$ from a univariate normal distribution with unknown mean µ and known variance $σ^2$. Derive the \n",
    "MLE estimator for the mean $µ$.\n",
    "\n",
    "$P(x_{1}, x_{2}.....x_{N}|µ) = \\prod_{i=1}^{N}P(x_{i}|µ) = \\prod_{i=1}^{N}\\frac{1}{\\sqrt{2π}σ}e^{-\\frac{(x_{i}-µ)^2}{2σ^2}}$<br>\n",
    "$ln(P(x_{1}, x_{2}.....x_{N}|µ)) = \\sum_{i=1}^{N}(ln\\frac{1}{\\sqrt{2π}σ}-\\frac{(x_{i}-µ)^2}{2σ^2})$<br>\n",
    "$\\frac{∂}{∂µ}P(x_{1}, x_{2}.....x_{N}|µ) = \\sum_{i=1}^{N}\\frac{x_{i}-µ}{σ^2}$<br>\n",
    "$\\frac{∂}{∂µ}P(x_{1}, x_{2}.....x_{N}|µ) = 0$<br>\n",
    "$\\sum_{i=1}^{N}\\frac{x_{i}-µ}{σ^2} = 0$<br>\n",
    "$\\sum_{i=1}^{N}(x_{i}-µ) = 0$<br>\n",
    "$\\sum_{i=1}^{N}x_{i} = nµ$<br>\n",
    "$µ_{MLE} = \\frac{\\sum_{i=1}^{N}x_{i}}{n}$<br>\n",
    "\n",
    "• Suppose that $X$ is a discrete random variable with the following probability mass function: where $0 ≤ θ ≤ 1$ is a parameter.\n",
    "\n",
    "|X|0|1|2|3|\n",
    "|-|-|-|-|-|\n",
    "|P(X)|2θ/3|θ/3|2(1-θ)/3|(1-θ)/3|\n",
    "\n",
    "The following $10$ independent observations were taken from such a distribution: $(3,0,2,1,3,2,1,0,2,1)$. What is the maximum likelihood estimate of $θ$.\n",
    "\n",
    "$Likelihood = L(θ) = \\prod_{i=1}^{n}P(X_{i}|θ) = P(X=3)P(X=0)P(X=2)P(X=1)P(X=3)P(X=2)P(X=1)P(X=0)P(X=2)P(X=1) = (2θ/3)^2(θ/3)^3(2(1-θ)/3)^3((1-θ)/3)^2$<br>\n",
    "$logL(θ) = \\sum_{i=1}^{N}logP(X_{i}|θ) = log(\\frac{(2^2)(1^3)(2^3)(1^2)}{(3^2)(3^3)(3^3)(3^2)}) + 5logθ + 5log(1-θ)$<br>\n",
    "$\\frac{∂logL(θ)}{∂θ} = 5/θ - 5/(1-θ) = 0$<br>\n",
    "Likelihood estimate of θ = 1/2\n",
    "\n",
    "\n",
    "### Naive Bayes\n",
    "A psychologist does a small survey on `happiness`. Each respondent provides a vector with entries $1$ or $0$ corresponding to whether they answer `yes` to a question or `no`, respectively. The question vector has attributes $x = (rich, married, healthy)$ Thus, a response $(1, 0, 1)$ would indicate that the respondent was `rich`, `unmarried`, `healthy`. In addition, each respondent gives a value $c = 1$ if they are content with their\n",
    "lifestyle, and $c = 0$ if they are not. The following responses were obtained from people who claimed also to be `content`: $(1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1)$ and for `not content`: $(0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0)$.\n",
    "\n",
    "<b>Prior Probability<b><br>\n",
    "$P(1) = N_{1}/(N_{0}+N_{1}) = 4/(5+4) = 4/9$<br>\n",
    "$P(0) = N_{0}/(N_{0}+N_{1}) = 5/(5+4) = 5/9$<br>\n",
    "(0 = not content, 1 = content)\n",
    "\n",
    "$P(rich|1) = P(healthy|1) = 3/4$<br>\n",
    "$P(not rich|1) = P(not healthy|1) = 1/4$<br>\n",
    "$P(married|1) = P(not married|1) = 1/2$\n",
    "\n",
    "$P(rich|0) = P(married|0) = P(healthy|0) = 1/5$<br>\n",
    "$P(not rich|0) = P(not married|0) = P(not healthy|0) = 4/5$\n",
    "\n",
    "• Using Naive Bayes, what is the probability that a person who is `not rich`, `married` and `healthy` is `content`?<br>\n",
    "<b>Probability of (not rich, married, healthy) is content: P(1|(not rich, married, healthy))<b><br>\n",
    "$P(1|(not rich, married, healthy))*P(not rich, married, healthy) = P((not rich, married, healthy)|1)*P(1)$<br>\n",
    "$P((not rich, married, healthy)|1)*P(1) = P(not rich|1)*P(married|1)*P(healthy|1)*P(1) = 1/4 * 2/4 * 3/4 * 4/9 = 1/24$\n",
    "\n",
    "$P(0|(not rich, married, healthy))*P(not rich, married, healthy) = P((not rich, married, healthy)|0)*P(0)$<br>\n",
    "$P((not rich, married, healthy)|0)*P(0) = P(not rich|0)*P(married|0)*P(healthy|0)*P(0) = 4/5 * 1/5 * 1/5 * 5/9 = 4/225$\n",
    "<br><br>\n",
    "$P(not rich, married, healthy) = P((not rich, married, healthy)|0)*P(0)+P((not rich, married, healthy)|1)*P(1) = 1/24 + 4/225 = 107/1800$<br>\n",
    "\n",
    "$P(1|(not rich, married, healthy) = (1/24)/(107/1800) = 0.7$\n",
    "\n",
    "• What is the probability that a person who is `not rich` and `married` is `content`?\n",
    "(That is, we do not know whether or not they are `healthy`.)<br>\n",
    "<b>Probability of (not rich, married) is content: P(1|(not rich, married))<b><br>\n",
    "$P(1|(not rich, married))*P(not rich, married) = P((not rich, married)|1)*P(1)$<br>\n",
    "$P((not rich, married)|1)*P(1) = 1/4 * 1/2 * 4/9 = 1/18$<br>\n",
    "$P(not rich, married) = P((not rich, married)|1)*P(1)+P((not rich, married)|0)*P(0) = 1/18 + (4/5 * 1/5 * 5/9) = 13/90$<br>\n",
    "$P(1|(not rich, married)) = (1/18)/(13/90) = 0.38$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Detection of Fake News\n",
    "### INTRODUCTION\n",
    "Over the last two years alone 90 percent of the data in the world was generated. Every minute 456,000 tweets are sent on Twitter, there are 2 billion active users on Facebook and each day worldwide 5 billion searches are made and 2.5 quintillion bytes of data is created. It is inevitable for fake news to gone viral in minutes. Is it possible to predict whether a news is fake only by looking at the news headlines? In this paper we will work on Naive Bayes Classification method based on Bag of Words (BoW) representation.\n",
    "### DATASET AND FEATURES\n",
    "1298 fake news headlines (which mostly include headlines of articles classified as biased etc.) and 1968 real news headlines, where the fake news headlines are from https://www.kaggle.com/mrisdal/fake-news/data and real news headlines are from https://www.kaggle.com/therohk/million-headlines have been compiled. Data is cleaned by removing words from fake news titles that are not a part of the headline, removing special characters from the headlines, and restricting real news headlines to those after October 2016 containing the word trump.\n",
    "\n",
    "### APPROACH\n",
    "#### Understanding the data\n",
    "In order to represent our data, Bag of Words approach is used. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. [1] In our model we will be using unigram and bigram:\n",
    "- Unigram: The occurrences of words in a document(frequency of the\n",
    "word).\n",
    "- Bigram: The occurrences of two adjacent words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✘ Fake News Headlines Statistics\n",
      "Total number of words:  3366\n",
      "Number of words with 1 occurrence:  1961\n",
      "Number of words with 2 occurrence:  536\n",
      "\n",
      "Most frequent 10 words:\n",
      " ('trump', 1130) ('the', 362) ('to', 344) ('donald', 195) ('in', 193) ('of', 189) ('for', 178) ('a', 167) ('and', 153) ('on', 144)\n",
      "\n",
      "Most frequent 10 words without STOP_WORDS:\n",
      " ('trump', 1130) ('donald', 195) ('hillary', 126) ('clinton', 113) ('just', 67) ('election', 61) ('new', 56) ('obama', 51) ('president', 50) ('win', 45)\n",
      "\n",
      "✔ Real News Headlines Statistics\n",
      "Total number of words:  3268\n",
      "Number of words with 1 occurrence:  1879\n",
      "Number of words with 2 occurrence:  564\n",
      "\n",
      "Most frequent 10 words:\n",
      " ('trump', 1484) ('donald', 709) ('to', 366) ('us', 186) ('trumps', 184) ('on', 178) ('in', 174) ('of', 157) ('says', 156) ('the', 152)\n",
      "\n",
      "Most frequent 10 words without STOP_WORDS:\n",
      " ('trump', 1484) ('donald', 709) ('trumps', 184) ('says', 156) ('clinton', 70) ('election', 69) ('north', 68) ('ban', 65) ('korea', 62) ('president', 61)\n",
      "\n",
      "A feature is considered important if it is common in real and uncommon in fake or common in fake and uncommon in real.\n",
      "\n",
      "✘ Features whose PRESENCE most strongly predicts that the news is FAKE: \n",
      "('u', (16, 0)) ('3', (16, 0)) ('7', (12, 0)) ('breaking', (24, 0)) ('black', (31, 1)) ('soros', (18, 0)) ('woman', (13, 0)) ('duke', (12, 0)) ('steal', (11, 0)) ('dr', (11, 0))\n",
      "\n",
      "✔ Features whose PRESENCE most strongly predicts that the news is REAL: \n",
      "('trumps', (4, 184)) ('north', (2, 68)) ('ban', (1, 65)) ('australia', (0, 31)) ('refugee', (0, 19)) ('climate', (0, 24)) ('turnbull', (0, 48)) ('paris', (0, 20)) ('travel', (0, 47)) ('korea', (0, 62))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "f_ug = {} # fake_unigram frequencies\n",
    "f_bg = {} # fake_bigram frequencies\n",
    "r_ug = {} # real_unigram frequencies\n",
    "r_bg = {} # real_bigram frequencies\n",
    "p_fake, p_real = 0, 0 # prior probabilities\n",
    "\n",
    "# FAKE NEWS HEADLINES TRAIN DATA\n",
    "for line in open(\"clean_fake-Train.txt\", \"r\"):\n",
    "    p_fake += 1\n",
    "    words = line.strip('\\n').split(' ')\n",
    "    for index in range(len(words)):\n",
    "        word = words[index]\n",
    "        r_ug[word] = 0\n",
    "        f_ug[word] = 1 if word not in f_ug else f_ug.get(word) + 1\n",
    "        if index != len(words)-1:\n",
    "            word += '-' + words[index+1]\n",
    "            r_bg[word] = 0\n",
    "            f_bg[word] = 1 if word not in f_bg else f_bg.get(word) + 1\n",
    "\n",
    "# REAL NEWS HEADLINES TRAIN DATA\n",
    "for line in open(\"clean_real-Train.txt\", \"r\"):\n",
    "    p_real += 1\n",
    "    words = line.strip('\\n').split(' ')\n",
    "    for index in range(len(words)):\n",
    "        word = words[index]\n",
    "        f_ug[word] = 0 if word not in f_ug else f_ug.get(word)\n",
    "        r_ug[word] = 1 if word not in r_ug else r_ug.get(word) + 1\n",
    "        if index != len(words)-1:\n",
    "            word += '-' + words[index+1]\n",
    "            f_bg[word] = 0 if word not in f_bg else f_bg.get(word)\n",
    "            r_bg[word] = 1 if word not in r_bg else r_bg.get(word) + 1\n",
    "\n",
    "# STATISTICS\n",
    "print(\"✘ Fake News Headlines Statistics\")\n",
    "print(\"Total number of words: \", len([i for i in f_ug.values() if i!=0]))\n",
    "print(\"Number of words with 1 occurrence: \", len([i for i in f_ug.values() if i==1]))\n",
    "print(\"Number of words with 2 occurrence: \", len([i for i in f_ug.values() if i==2]))\n",
    "print(\"\\nMost frequent 10 words:\\n\", *sorted(f_ug.items(), key=operator.itemgetter(1))[-10:][::-1])\n",
    "\n",
    "f_ug_sw = {}\n",
    "r_ug_sw = {}\n",
    "for word, freq in f_ug.items():\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        f_ug_sw[word] = freq\n",
    "        r_ug_sw[word] = r_ug.get(word)\n",
    "\n",
    "print(\"\\nMost frequent 10 words without STOP_WORDS:\\n\", *sorted(f_ug_sw.items(), key=operator.itemgetter(1))[-10:][::-1])\n",
    "\n",
    "print(\"\\n✔ Real News Headlines Statistics\")\n",
    "print(\"Total number of words: \", len([i for i in r_ug.values() if i!=0]))\n",
    "print(\"Number of words with 1 occurrence: \", len([i for i in r_ug.values() if i==1]))\n",
    "print(\"Number of words with 2 occurrence: \", len([i for i in r_ug.values() if i==2]))\n",
    "print(\"\\nMost frequent 10 words:\\n\", *sorted(r_ug.items(), key=operator.itemgetter(1))[-10:][::-1])\n",
    "print(\"\\nMost frequent 10 words without STOP_WORDS:\\n\", *sorted(r_ug_sw.items(), key=operator.itemgetter(1))[-10:][::-1])\n",
    "\n",
    "f_presence = {}\n",
    "r_presence = {}\n",
    "for word,freq in f_ug.items():\n",
    "    nom = freq\n",
    "    denom = r_ug.get(word)\n",
    "    if nom>denom:\n",
    "        if (nom+1)/(denom+1)>11:\n",
    "            f_presence[word] = (nom,denom)\n",
    "    elif (denom+1)/(nom+1)>19:\n",
    "        r_presence[word] = (nom,denom)\n",
    "        \n",
    "print(\"\\nA feature is considered important if it is common in real and uncommon in fake or common in fake and uncommon in real.\")\n",
    "print(\"\\n✘ Features whose PRESENCE most strongly predicts that the news is FAKE: \")\n",
    "print(*f_presence.items())\n",
    "print(\"\\n✔ Features whose PRESENCE most strongly predicts that the news is REAL: \")\n",
    "print(*r_presence.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Naive Bayes\n",
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Naive Bayes classifiers are built on Bayesian classification methods. These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$. [2] Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "$$P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}$$\n",
    "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "$$\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}$$\n",
    "- Disadvantages: It assumes every feature is independent\n",
    "- Probability Computation: Log Probabilities are computed in order to prevent numerical underflow and Laplace smoothing are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIOR PROBABILITIES\n",
    "p_fake = p_fake/(p_fake+p_real)\n",
    "p_real = 1 - p_fake\n",
    "\n",
    "# if stop_words is True: STOP WORDS is extracted\n",
    "stop_words = False\n",
    "if stop_words:\n",
    "    r_ug = r_ug_sw\n",
    "    f_ug = f_ug_sw\n",
    "\n",
    "# COMPUTING PROBABILITIES\n",
    "f_ug_denom = sum(f_ug.values()) + len(f_ug)\n",
    "r_ug_denom = sum(r_ug.values()) + len(r_ug)\n",
    "for word, f in f_ug.items():\n",
    "    f_ug[word] = np.log10((f+1) / f_ug_denom)\n",
    "    r_ug[word] = np.log10((r_ug.get(word)+1) / r_ug_denom)\n",
    "f_bg_denom = sum(f_bg.values()) + len(f_bg)\n",
    "r_bg_denom = sum(r_bg.values()) + len(r_bg)\n",
    "for word, f in f_bg.items():\n",
    "    f_bg[word] = np.log10((f+1) / f_bg_denom)\n",
    "    r_bg[word] = np.log10((r_bg.get(word)+1) / r_bg_denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N=1 (UNIGRAM), N=2 (BIGRAM) and N=1,2 Implementation <br>\n",
    "For each class (fake and real) log probabilities are taken from dictionary for each word of test headline and summed. After adding prior probabilities to each sum, larger sum is chosen as our prediction of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram():\n",
    "    hit, count = 0, -1\n",
    "    for line in open(\"test.csv\", \"r\"):\n",
    "        count += 1\n",
    "        if count == 0: continue\n",
    "        words = line.split(',')[0].split(' ')\n",
    "        label = line.split(',')[1].strip('\\n')\n",
    "        fake, real = p_fake, p_real\n",
    "        for word in words:\n",
    "            fake += f_ug.get(word) if word in f_ug else 0\n",
    "            real += r_ug.get(word) if word in r_ug else 0\n",
    "        hit+=1 if fake>real and label==\"fake\" else 1 if real>=fake and label==\"real\" else 0\n",
    "    print(\"UNIGRAM accuracy: %{}\".format(hit*100/count))\n",
    "\n",
    "def bigram():\n",
    "    hit, count = 0, -1\n",
    "    for line in open(\"test.csv\", \"r\"):\n",
    "        count += 1\n",
    "        if count == 0: continue\n",
    "        words = line.split(',')[0].split(' ')\n",
    "        label = line.split(',')[1].strip('\\n')\n",
    "        fake, real = p_fake, p_real\n",
    "        words = [words[i]+'-'+words[i+1] for i in range(len(words)-1)]\n",
    "        for word in words:\n",
    "            fake += f_bg.get(word) if word in f_bg else 0\n",
    "            real += r_bg.get(word) if word in r_bg else 0\n",
    "        hit+=1 if fake>real and label==\"fake\" else 1 if real>=fake and label==\"real\" else 0\n",
    "    print(\"BIGRAM accuracy: %{}\".format(hit * 100 / count))\n",
    "    \n",
    "def unigram_bigram():\n",
    "    hit, count = 0, -1\n",
    "    for line in open(\"test.csv\", \"r\"):\n",
    "        count += 1\n",
    "        if count == 0: continue\n",
    "        words = line.split(',')[0].split(' ')\n",
    "        label = line.split(',')[1].strip('\\n')\n",
    "        fake, real = p_fake, p_real\n",
    "        for word in words:\n",
    "            fake += f_ug.get(word) if word in f_ug else 0\n",
    "            real += r_ug.get(word) if word in r_ug else 0\n",
    "        words = [words[i]+'-'+words[i+1] for i in range(len(words)-1)]\n",
    "        for word in words:\n",
    "            fake += f_bg.get(word) if word in f_bg else 0\n",
    "            real += r_bg.get(word) if word in r_bg else 0\n",
    "        hit+=1 if fake>real and label==\"fake\" else 1 if real>=fake and label==\"real\" else 0\n",
    "    print(\"UNIGRAM+BIGRAM accuracy: %{}\".format(hit * 100 / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing effect of the words on prediction\n",
    "- Using Feature Extraction Method: Tfidf - Term Frequency times Inverse Document Frequency: The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. [3] The more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. <br>\n",
    "- Stop Words: Frequently occurring words that don't carry much meaning like 'the', 'a', 'is' are called stopwords. Extracting stopwords from vocabulary improves feature extraction. But our test results shows extracting stop words actually worsen the scores. It may happened since we are only observing one sentence as our document not like sentences as review sentiment analysis. Same occurs when implementing bigram model, after extracting stop words there is not left any pairs for both documents. Unigram model is considered to be more success when compared to bigram since pairing headlines actualy lessen the data. But if we unite two models as N=1,2 we are expecting to have better results. While Naive Bayes CLassification is fairly simple and fast to execute algorithm it models our data well. We can try to improve our system by playing with vocabulary. There are words that explains the fake headlines better and there are words that explains the real headlines better. If we are to unite both words into one vocabulary we expect to have better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features whose PRESENCE most strongly predicts that the news is REAL (with highest tfidf):\n",
      "['no', 'day', 'great', 'major', 'head', 'job', 'tower', 'punch', '30', 'neck']\n",
      "\n",
      "Features whose ABSENCE most strongly predicts that the news is REAL (with lowest tfidf):\n",
      "['trump', 'us', 'trumps', 'on', 'says', 'donald', 'with', 'clinton', 'election', 'north']\n",
      "\n",
      "Features whose PRESENCE most strongly predicts that the news is FAKE (with highest tfidf):\n",
      "['looking', 'goals', 'independents', 'have', 'collett', 'comments', 'cops', 'ha', 'lock', 'kanye']\n",
      "\n",
      "Features whose ABSENCE most strongly predicts that the news is FAKE (with lowest tfidf):\n",
      "['trump', 'donald', 'hillary', 'with', 'election', 'america', 'win', 'be', 'has', 'victory']\n",
      "\n",
      "Non-stopwords that most strongly predict that the news is REAL (with highest tfidf):\n",
      "['wants', 'day', 'great', 'major', 'head', 'job', 'punch', 'tower', '30', 'neck']\n",
      "\n",
      "Non-stopwords that most strongly predict that the news is FAKE (with highest tfidf):\n",
      "['crowds', 'independents', 'saying', 'electors', 'kanye', 'ha', 'cops', 'lock', 'collett', 'comments']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer and TfidfTransformer can be done in the same step with TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "r_train_set = [line.strip('\\n') for line in open(\"clean_real-Train.txt\", \"r\")]\n",
    "f_train_set = [line.strip('\\n') for line in open(\"clean_fake-Train.txt\", \"r\")]\n",
    "test_set = [line.split(',')[0] for line in open(\"test.csv\", \"r\")]\n",
    "\n",
    "# WITH STOP WORDS\n",
    "# REAL NEWS\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None,\n",
    "                             ngram_range=(1,1), analyzer='word',\n",
    "                             min_df=1, max_df=1.0)\n",
    "train = vectorizer.fit_transform(r_train_set)\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "sort_by_tfidf = train.max(axis=0).toarray().ravel().argsort()\n",
    "print(\"\\nFeatures whose PRESENCE most strongly predicts that the news is REAL (with highest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[-10:]])\n",
    "print(\"\\nFeatures whose ABSENCE most strongly predicts that the news is REAL (with lowest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[:10]])\n",
    "\n",
    "# FAKE NEWS\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None,\n",
    "                             ngram_range=(1,1), analyzer='word',\n",
    "                             min_df=1, max_df=1.0)\n",
    "train = vectorizer.fit_transform(f_train_set)\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "sort_by_tfidf = train.max(axis=0).toarray().ravel().argsort()\n",
    "print(\"\\nFeatures whose PRESENCE most strongly predicts that the news is FAKE (with highest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[-10:]])\n",
    "print(\"\\nFeatures whose ABSENCE most strongly predicts that the news is FAKE (with lowest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[:10]])\n",
    "\n",
    "# WITHOUT STOP WORDS\n",
    "# REAL NEWS\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None,\n",
    "                             ngram_range=(1,1), analyzer='word',\n",
    "                             min_df=1, max_df=1.0, stop_words='english')\n",
    "train = vectorizer.fit_transform(r_train_set)\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "sort_by_tfidf = train.max(axis=0).toarray().ravel().argsort()\n",
    "print(\"\\nNon-stopwords that most strongly predict that the news is REAL (with highest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[-10:]])\n",
    "\n",
    "# FAKE NEWS\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None,\n",
    "                             ngram_range=(1,1), analyzer='word',\n",
    "                             min_df=1, max_df=1.0, stop_words='english')\n",
    "train = vectorizer.fit_transform(f_train_set)\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "sort_by_tfidf = train.max(axis=0).toarray().ravel().argsort()\n",
    "\n",
    "print(\"\\nNon-stopwords that most strongly predict that the news is FAKE (with highest tfidf):\")\n",
    "print([features[i] for i in sort_by_tfidf[-10:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS AND CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM accuracy: %86.70756646216769\n",
      "BIGRAM accuracy: %83.02658486707567\n",
      "UNIGRAM+BIGRAM accuracy: %88.13905930470348\n"
     ]
    }
   ],
   "source": [
    "unigram()\n",
    "bigram()\n",
    "unigram_bigram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper a Fake News Detection System is built using Naive Bayes Classification Algorithm with Bag Of Words representation and feature extraction methods are examined. Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering and recommendation systems. They are extremely fast for both training and prediction, they provide straightforward probabilistic prediction and they are often very easily interpretable. We see from our test results that n=1,2 model is proven to be the most success as expected. Overall, basics of natural language processing and bayesian probability is examined and implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES\n",
    "[1] https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words <br>\n",
    "[2] https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb <br>\n",
    "[3] https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/feature_extraction/text.py#L1365 <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
